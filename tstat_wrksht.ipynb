{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No-Regret and Bag Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_list = ['ALL_1_36_48_12_RF_label_really_correct',\n",
    "             'ALL_1_36_48_12_S_all_BagRandom',\n",
    "             'ALL_1_36_48_12_No_Regret',\n",
    "             'ALL_1_36_48_12_No_Regret_T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = pd.read_excel('icews_data.xlsx',sheet_name = 'baselines')\n",
    "country_list = list(set(country_df['country']))\n",
    "country_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('icews_data.xlsx',sheet_name = 'test_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results by Baseline by Country (one at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = 'ALL_1_36_48_12_S_all_trim_Bag'\n",
    "df_country = df[(df['test_run']==test_run)]\n",
    "    \n",
    "for baseline in baseline_list:\n",
    "    df_baseline = df[(df['test_run']==baseline)]\n",
    "    for country in country_list:\n",
    "        df_country_temp = df_country[(df_country['country']==country)]\n",
    "        df_baseline_temp = df_baseline[(df_baseline['country']==country)]\n",
    "        acc_arr = np.array(df_country_temp['acc'])\n",
    "        bas_arr = np.array(df_baseline_temp['acc'])\n",
    "        results = stats.ttest_rel(acc_arr,bas_arr,alternative=\"greater\")\n",
    "        f = open('ttest_by_country.csv','a')\n",
    "        f.write(baseline + \",\" + \n",
    "                country + \",\" + \n",
    "                str(acc_arr.mean()) + \",\" + \n",
    "                str(bas_arr.mean()) + \",\" + \n",
    "                str(results[1]) + \"\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results by Baseline Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1520,)\n",
      "(1520,)\n",
      "(1520,)\n",
      "(1520,)\n",
      "(1520,)\n",
      "(1520,)\n",
      "(1520,)\n",
      "(1520,)\n"
     ]
    }
   ],
   "source": [
    "test_run = 'ALL_1_36_48_12_S_all_trim_Bag'\n",
    "df_country = df[(df['test_run']==test_run)]\n",
    "    \n",
    "for baseline in baseline_list:\n",
    "    df_baseline = df[(df['test_run']==baseline)]\n",
    "    acc_arr = np.array(df_country['acc'])\n",
    "    bas_arr = np.array(df_baseline['acc'])\n",
    "    print(acc_arr.shape)\n",
    "    print(bas_arr.shape)\n",
    "    results = stats.ttest_rel(acc_arr,bas_arr,alternative=\"greater\")\n",
    "    f = open('ttest_overall.csv','a')\n",
    "    f.write(baseline + \",\" + \n",
    "                country + \",\" + \n",
    "                str(acc_arr.mean()) + \",\" + \n",
    "                str(bas_arr.mean()) + \",\" + \n",
    "                str(results[1]) + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_arr = np.array(df_baselines['nve'])\n",
    "bas_arr = np.array(df_baselines['maj'])\n",
    "results = stats.ttest_rel(acc_arr,bas_arr,alternative=\"greater\")\n",
    "f = open('ttest_icews.csv','a')\n",
    "f.write(\"naive/majority\" + \",\" + \n",
    "                \"icews\" + \",\" + \n",
    "                str(acc_arr.mean()) + \",\" + \n",
    "                str(bas_arr.mean()) + \",\" + \n",
    "                str(results[1]) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for row in range(df_baselines.shape[0]):\n",
    "    country = df_baselines['country'][row]\n",
    "    naive_acc = df_baselines['nve'][row]\n",
    "    majority_acc = df_baselines['maj'][row]\n",
    "    d.setdefault(df_baselines['country'][row],{'naive' : -1, 'majority' : -1})\n",
    "    d[country]['naive'] = naive_acc\n",
    "    d[country]['majority'] = majority_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Baseline by Country (individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m test_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALL_1_36_48_12_S_all_trim_Bag\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m df_test_run \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_run\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mtest_run)]\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m country,data \u001b[38;5;129;01min\u001b[39;00m \u001b[43md\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m baseline \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnaive\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajority\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      7\u001b[0m         df_country \u001b[38;5;241m=\u001b[39m df_test_run[(df_test_run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mcountry)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('icews_data.xlsx',sheet_name = 'test_data')\n",
    "test_run = 'ALL_1_36_48_12_S_all_trim_Bag'\n",
    "df_test_run = df[(df['test_run']==test_run)]\n",
    "        \n",
    "for country,data in d.items():\n",
    "    for baseline in ['naive','majority']:\n",
    "        df_country = df_test_run[(df_test_run['country']==country)]\n",
    "        acc_arr = np.array(df_country['acc'])\n",
    "        results = stats.ttest_1samp(acc_arr,d[country][baseline],alternative=\"greater\")\n",
    "        f = open('ttest_by_country.csv','a')\n",
    "        f.write(baseline + \",\" + \n",
    "                country + \",\" + \n",
    "                str(acc_arr.mean()) + \",\" + \n",
    "                str(d[country][baseline]) + \",\" + \n",
    "                str(results[1]) + \"\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Baseline by Overall Country-Baseline Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('icews_data.xlsx',sheet_name = 'test_data')\n",
    "test_run = 'ALL_1_36_48_12_S_all_trim_Bag'\n",
    "df_test_run = df[(df['test_run']==test_run)]\n",
    "term_dict = {'naive':'nve','majority':'maj'}\n",
    "        \n",
    "\n",
    "for baseline in ['naive','majority']:\n",
    "    acc_arr = np.array(df_test_run['acc'])\n",
    "    bas_arr = np.array(df_test_run[term_dict[baseline]])\n",
    "    results = stats.ttest_rel(acc_arr,bas_arr,alternative=\"greater\")\n",
    "    f = open('ttest_icews.csv','a')\n",
    "    f.write(baseline + \",\" + \n",
    "                \"icews\" + \",\" + \n",
    "                str(acc_arr.mean()) + \",\" + \n",
    "                str(bas_arr.mean()) + \",\" + \n",
    "                str(results[1]) + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('icews_data.xlsx',sheet_name = 'baselines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_arr = np.array(df['accuracy'])\n",
    "bas_arr = np.array(df['nve'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=1.918934892230754, pvalue=0.02843952408709914)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_rel(acc_arr,bas_arr,alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S v D v D&MD v ALL by [Bag,ESN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subSetL = ['S','D','D&ND','ALL']\n",
    "df = pd.read_excel('proposal results (cow).xlsx',sheet_name = 'ESN_keds')\n",
    "for idx,subSet1 in enumerate(subSetL):\n",
    "    for subSet2 in subSetL:\n",
    "        df_subset_1 = df[(df['subset']==subSet1)]\n",
    "        df_subset_2 = df[(df['subset']==subSet2)]\n",
    "        acc_arr_1 = np.array(df_subset_1['accuracy'])\n",
    "        acc_arr_2 = np.array(df_subset_2['accuracy'])\n",
    "        results = stats.ttest_rel(acc_arr_1,acc_arr_2,alternative=\"greater\")\n",
    "        print(subSet1,subSet2,round(acc_arr_1.mean(),3),round(acc_arr_2.mean(),3),round(results[1],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S v D v D&MD v ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#dataSetL = ['somalia','cuba','combined','haiti','india','eurovision','bitcoin_otc','bitcoin_alpha']\n",
    "subSetL = ['S','D','D&ND','ALL']\n",
    "df = pd.read_excel('proposal results (cow).xlsx',sheet_name = 'Combined')\n",
    "for subSet1 in subSetL:\n",
    "    for subSet2 in subSetL:\n",
    "        df_subset_1 = df[(df['subset']==subSet1)]\n",
    "        df_subset_2 = df[(df['subset']==subSet2)]\n",
    "        acc_arr_1 = np.array(df_subset_1['accuracy'])\n",
    "        acc_arr_2 = np.array(df_subset_2['accuracy'])\n",
    "        print(subSet1,subSet2,acc_arr_1.mean(),acc_arr_2.mean(),stats.ttest_rel(acc_arr_1,acc_arr_2,alternative=\"greater\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL v GRAPH v GAME by [Bag,ESN] X [S,D,D&MD,ALL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#dataSetL = ['somalia','cuba','combined','haiti','india','eurovision','bitcoin_otc','bitcoin_alpha']\n",
    "gameStratL = ['ALL','GRAPH','GAME']\n",
    "subSetL = ['S','D','D&ND','ALL']\n",
    "modelL = ['ESN','Bag']\n",
    "for gmSt1 in gameStratL:\n",
    "    df_1 = pd.read_excel('proposal results (cow).xlsx',sheet_name = gmSt1)\n",
    "    for gmSt2 in gameStratL:\n",
    "        if gmSt1 == gmSt2:\n",
    "            continue\n",
    "        df_2 = pd.read_excel('proposal results (cow).xlsx',sheet_name = gmSt2)\n",
    "        for mdl in modelL:\n",
    "            for sS in subSetL:\n",
    "                df_subset_1 = df_1[(df_1['subset']==sS)&(df_1['method']==mdl)]\n",
    "                df_subset_2 = df_2[(df_2['subset']==sS)&(df_2['method']==mdl)]\n",
    "                acc_arr_1 = np.array(df_subset_1['accuracy'])\n",
    "                acc_arr_2 = np.array(df_subset_2['accuracy'])\n",
    "                results = stats.ttest_rel(acc_arr_1,acc_arr_2,alternative=\"greater\")\n",
    "                print(sS,mdl,gmSt1,gmSt2,round(acc_arr_1.mean(),3),round(acc_arr_2.mean(),3),round(results[1],3)),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL v GRAPH v GAME by [Bag,ESN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gameStratL = ['ALL','GRAPH','GAME']\n",
    "modelL = ['ESN','Bag']\n",
    "for gmSt1 in gameStratL:\n",
    "    df_1 = pd.read_excel('proposal results (cow).xlsx',sheet_name = gmSt1)\n",
    "    for gmSt2 in gameStratL:\n",
    "        if gmSt1 == gmSt2:\n",
    "            continue\n",
    "        df_2 = pd.read_excel('proposal results (cow).xlsx',sheet_name = gmSt2)\n",
    "        for mdl in modelL:\n",
    "            df_subset_1 = df_1[(df_1['method']==mdl)]\n",
    "            df_subset_2 = df_2[(df_2['method']==mdl)]\n",
    "            acc_arr_1 = np.array(df_subset_1['accuracy'])\n",
    "            acc_arr_2 = np.array(df_subset_2['accuracy'])\n",
    "            print(mdl,gmSt1,gmSt2,acc_arr_1.mean(),acc_arr_2.mean(),stats.ttest_rel(acc_arr_1,acc_arr_2,alternative=\"greater\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL v GRAPH v GAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gameStratL = ['ALL','GRAPH','GAME']\n",
    "for gmSt1 in gameStratL:\n",
    "    df_1 = pd.read_excel('proposal results (cow).xlsx',sheet_name = gmSt1)\n",
    "    for gmSt2 in gameStratL:\n",
    "        if gmSt1 == gmSt2:\n",
    "            continue\n",
    "        df_2 = pd.read_excel('proposal results (cow).xlsx',sheet_name = gmSt2)\n",
    "        acc_arr_1 = np.array(df_1['accuracy'])\n",
    "        acc_arr_2 = np.array(df_2['accuracy'])\n",
    "        print(gmSt1,gmSt2,acc_arr_1.mean(),acc_arr_2.mean(),stats.ttest_rel(acc_arr_1,acc_arr_2,alternative=\"greater\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
